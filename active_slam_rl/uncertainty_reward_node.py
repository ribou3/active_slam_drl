import rclpy
from rclpy.node import Node
# ÈÄö‰ø°ÂìÅË≥™(QoS)„ÅÆË®≠ÂÆöÁî®: SLAM„Åã„Çâ„ÅÆ„Éá„Éº„Çø„ÇíÂèñ„Çä„Åì„Åº„Åï„Å™„ÅÑ„Åü„ÇÅ„Å´ÂøÖË¶Å
from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy, DurabilityPolicy
from geometry_msgs.msg import PoseWithCovarianceStamped
from std_msgs.msg import Float32
import numpy as np

class UncertaintyRewardNode(Node):
    def __init__(self):
        super().__init__('uncertainty_reward_node')
        
        # ---------------------------------------------------------
        # 1. QoS (Quality of Service) „ÅÆË®≠ÂÆö
        # ---------------------------------------------------------
        # SLAM„Éé„Éº„Éâ„ÅØÈÄöÂ∏∏ "Best Effort" (Â±ä„Åã„Å™„Åè„Å¶„ÇÇÂÜçÈÄÅ„Åó„Å™„ÅÑ) „Åß„Éá„Éº„Çø„ÇíÈÄÅ„Çä„Åæ„Åô„ÄÇ
        # „Åì„Å°„Çâ„Åå "Reliable" (Á¢∫ÂÆüÊÄßÈáçË¶ñ) „ÅßÂæÖ„Å£„Å¶„ÅÑ„Çã„Å®„ÄÅË®≠ÂÆö‰∏ç‰∏ÄËá¥„Åß„Éá„Éº„Çø„ÅåÊù•„Åæ„Åõ„Çì„ÄÇ
        # „Åù„ÅÆ„Åü„ÇÅ„ÄÅ„Å©„Çì„Å™Áõ∏Êâã„Å®„ÇÇÈÄö‰ø°„Åß„Åç„Çã„ÄåÊúÄÂº∑„ÅÆÂèó„ÅëÂÖ•„ÇåÊÖãÂã¢„Äç„Çí‰Ωú„Çä„Åæ„Åô„ÄÇ
        qos_profile = QoSProfile(
            reliability=ReliabilityPolicy.BEST_EFFORT,
            durability=DurabilityPolicy.VOLATILE, # ÊúÄÊñ∞„ÅÆ„Éá„Éº„Çø„ÅÆ„ÅøÊ¨≤„Åó„ÅÑ
            history=HistoryPolicy.KEEP_LAST,
            depth=10
        )

        # ---------------------------------------------------------
        # 2. „Éà„Éî„ÉÉ„ÇØ„ÅÆË≥ºË™≠ (Subscriber)
        # ---------------------------------------------------------
        # Ëá™Â∑±‰ΩçÁΩÆ„Å®„ÄåÂÖ±ÂàÜÊï£Ë°åÂàó(‰∏çÁ¢∫„Åã„Åï)„Äç„ÇíÂèó„ÅëÂèñ„Çä„Åæ„Åô„ÄÇ
        # „Éà„Éî„ÉÉ„ÇØÂêç: '/pose' (Áí∞Â¢É„Å´„Çà„Å£„Å¶„ÅØ /amcl_pose „ÇÑ /slam_toolbox/pose „ÅÆÂ†¥Âêà„ÇÇ)
        self.pose_sub = self.create_subscription(
            PoseWithCovarianceStamped,
            '/pose',
            self.pose_callback,
            qos_profile
        )
        
        # ---------------------------------------------------------
        # 3. Â†±ÈÖ¨„ÅÆÈÖç‰ø° (Publisher)
        # ---------------------------------------------------------
        # Ë®àÁÆó„Åó„Åü„Äå‰∏çÁ¢∫„Åã„ÅïÂ†±ÈÖ¨„Äç„ÇíÂ≠¶Áøí„Ç®„Éº„Ç∏„Çß„É≥„Éà„Å´ÈÄÅ„Çä„Åæ„Åô„ÄÇ
        self.reward_pub = self.create_publisher(Float32, '/uncertainty_reward', 10)
        
        # ---------------------------------------------------------
        # [cite_start]4. Ë´ñÊñá„Éë„É©„É°„Éº„Çø„ÅÆË®≠ÂÆö (Table 2„Çà„Çä [cite: 230])
        # ---------------------------------------------------------
        # Œ∑ („Ç§„Éº„Çø): Â†±ÈÖ¨Èñ¢Êï∞„ÅÆ„Çπ„Ç±„Éº„É™„É≥„Ç∞‰øÇÊï∞„ÄÇË´ñÊñá„Åß„ÅØ 0.01
        self.eta = 0.01  
        
        # l („Ç®„É´): Áä∂ÊÖãÁ©∫Èñì„ÅÆÊ¨°ÂÖÉÊï∞„ÄÇ
        # [cite_start]2DÂπ≥Èù¢ÁßªÂãï„É≠„Éú„ÉÉ„Éà (x, y, yaw) „Å™„ÅÆ„Åß 3Ê¨°ÂÖÉ [cite: 31]
        self.dim_l = 3.0 
        
        # ÂÆâÂÖ®Ë£ÖÁΩÆÁî®„ÅÆÈñæÂÄ§: ÂÖ±ÂàÜÊï£„ÅÆÂêàË®à„Åå„Åì„Çå„ÇíË∂Ö„Åà„Åü„ÇâSLAMÂ¥©Â£ä„Å®„Åø„Å™„Åô
        self.sigma_sum_threshold = 100.0

        # Ëµ∑ÂãïÁ¢∫Ë™ç„É≠„Ç∞
        self.get_logger().info("‚úÖ D-opt Reward Node Started (Waiting for /pose data...)")

    def pose_callback(self, msg):
        """
        Ëá™Â∑±‰ΩçÁΩÆ„ÅÆÂÖ±ÂàÜÊï£(Œ£)„ÇíÂèó„ÅëÂèñ„Çä„ÄÅD-optimality„Å´Âü∫„Å•„ÅÑ„ÅüÂ†±ÈÖ¨„ÇíË®àÁÆó„Åô„Çã
        """
        
        # ‚òÖ ÁîüÂ≠òÁ¢∫Ë™ç„É≠„Ç∞: „Åì„Çå„ÅåÂá∫„Çå„Å∞„Éá„Éº„Çø„ÅØÂ±ä„ÅÑ„Å¶„ÅÑ„Åæ„Åô
        self.get_logger().info("üì® Message Received!", throttle_duration_sec=2.0)

        # ---------------------------------------------------------
        # [cite_start]„Çπ„ÉÜ„ÉÉ„Éó 1: ÂÖ±ÂàÜÊï£Ë°åÂàó„ÅÆÊï¥ÂΩ¢ [cite: 209-213]
        # ---------------------------------------------------------
        # ROS„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÅØ‰∏ÄÂàó„ÅÆ„É™„Çπ„Éà(36Ë¶ÅÁ¥†)„Å™„ÅÆ„Åß„ÄÅ6x6Ë°åÂàó„Å´Â§âÊèõ
        cov_6x6 = np.array(msg.pose.covariance).reshape(6, 6)
        
        # ---------------------------------------------------------
        # [cite_start]„Çπ„ÉÜ„ÉÉ„Éó 2: ÂøÖË¶Å„Å™ÊàêÂàÜ„ÅÆÊäΩÂá∫ (2DÁßªÂãïÁî®) [cite: 483-485]
        # ---------------------------------------------------------
        # Ë°åÂàó„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ: 0:X, 1:Y, 5:Yaw (ÂõûËª¢)
        # TurtleBot„ÅØÂ∫ä„ÅÆ‰∏ä„ÇíËµ∞„Çã„ÅÆ„ÅßZËª∏„Å™„Å©„ÅØÁÑ°Ë¶ñ„Åó„Åæ„Åô
        indices = [0, 1, 5]
        sigma = cov_6x6[np.ix_(indices, indices)]
        
        # ---------------------------------------------------------
        # ‚òÖ ÂÆâÂÖ®Ë£ÖÁΩÆ 1: Êï∞ÂÄ§Áï∞Â∏∏„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ
        # ---------------------------------------------------------
        # NaN(ÈùûÊï∞)„ÇÑInf(ÁÑ°ÈôêÂ§ß)„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åü„ÇâË®àÁÆó‰∏çÂèØ„Å™„ÅÆ„Åß„É™„Çª„ÉÉ„Éà
        if np.any(np.isnan(sigma)) or np.any(np.isinf(sigma)):
            self.get_logger().error("‚ùå Math Error: Sigma contains NaN or Inf!")
            self._publish_reward(0.0)
            return

        # ÂÖ±ÂàÜÊï£„ÅåÂ§ß„Åç„Åô„Åé„Çã(ÔºùÂÆåÂÖ®„Å´Ëø∑Â≠ê)Â†¥Âêà„ÅØ„Ç®„É©„Éº„Å®„Åó„Å¶Âá¶ÁêÜ
        sigma_sum = np.sum(np.abs(sigma))
        if sigma_sum > self.sigma_sum_threshold:
            self.get_logger().error(f"‚ùå Critical: Sigma sum too large ({sigma_sum:.2f})")
            self._publish_reward(0.0)
            return

        # ---------------------------------------------------------
        # [cite_start]„Çπ„ÉÜ„ÉÉ„Éó 3: D-optimality (DÊúÄÈÅ©ÊÄßÂü∫Ê∫ñ) „ÅÆË®àÁÆó [cite: 36-46]
        # ---------------------------------------------------------
        # ÂÆöÁæ©: D-opt = exp( 1/l * Œ£ log(Œª_k) ) 
        # ÊÑèÂë≥: ‰∏çÁ¢∫„Åã„Åï„ÅÆÊ•ïÂÜÜ‰Ωì„ÅÆ‰ΩìÁ©ç„ÄÇÂ∞è„Åï„ÅÑ„Åª„Å©Ëá™‰ø°„Åå„ÅÇ„Çã„ÄÇ
        
        try:
            # Âõ∫ÊúâÂÄ§ (Œª) „ÇíË®àÁÆó
            eig_vals = np.linalg.eigvals(sigma)
            
            # log(0)„ÇíÈò≤„Åê„Åü„ÇÅ„ÅÆÊï∞ÂÄ§ÂÆâÂÆöÂåñ (1e-9Êú™Ê∫Ä„ÅØ1e-9„Å´„Åô„Çã)
            eig_vals = np.maximum(eig_vals, 1e-9)
            
            # Âºè(3)„ÅÆÂÆüË£Ö
            log_sum = np.sum(np.log(eig_vals))
            d_opt = np.exp(log_sum / self.dim_l)
            
            # ---------------------------------------------------------
            # [cite_start]„Çπ„ÉÜ„ÉÉ„Éó 4: Â†±ÈÖ¨„Å∏„ÅÆÂ§âÊèõ (Intrinsic Reward) [cite: 196-201]
            # ---------------------------------------------------------
            # Âºè(12): R_u = tanh( Œ∑ / D-opt )
            # D-opt(‰∏çÁ¢∫„Åã„Åï)„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©„ÄÅÂ†±ÈÖ¨„ÅØ 1.0 „Å´Ëøë„Å•„Åè
            
            if d_opt > 1e-9:
                tanh_input = self.eta / d_opt
                intrinsic_reward = np.tanh(tanh_input)
            else:
                # ‰∏çÁ¢∫„Åã„Åï„Åå„Åª„Åº0„Å™„ÇâÊúÄÂ§ßÂ†±ÈÖ¨
                intrinsic_reward = 1.0 
            
            # ---------------------------------------------------------
            # ‚òÖ ÂÆâÂÖ®Ë£ÖÁΩÆ 2: Â†±ÈÖ¨ÂÄ§„ÅÆ„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞
            # ---------------------------------------------------------
            # tanh„ÅÆÊï∞Â≠¶ÁöÑÊÄßË≥™‰∏ä 1.0 „ÇíË∂Ö„Åà„Çã„Åì„Å®„ÅØ„Å™„ÅÑ„Åå„ÄÅÂøµ„ÅÆ„Åü„ÇÅ„Ç¨„Éº„Éâ
            if intrinsic_reward > 1.0:
                self.get_logger().warn(f"‚ö†Ô∏è Reward saturated: {intrinsic_reward}")
                intrinsic_reward = 1.0

            # ---------------------------------------------------------
            # ‚òÖ Á¢∫Ë™çÁî®„É≠„Ç∞ (2Áßí„Å´1ÂõûË°®Á§∫)
            # ---------------------------------------------------------
            self.get_logger().info(
                f"‚úÖ Eigs: {np.round(eig_vals, 5)} | D-opt: {d_opt:.6f} | Reward: {intrinsic_reward:.6f}",
                throttle_duration_sec=2.0
            )
            
            self._publish_reward(intrinsic_reward)
            
        except Exception as e:
            self.get_logger().warn(f"Calculation Error: {e}")
            self._publish_reward(0.0)

    def _publish_reward(self, value):
        out_msg = Float32()
        out_msg.data = float(value)
        self.reward_pub.publish(out_msg)

def main(args=None):
    rclpy.init(args=args)
    node = UncertaintyRewardNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()